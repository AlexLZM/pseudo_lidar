# Introduction to Pseudo Lidar
Author: Tianxiang Zhou, Zhimin Lyu

## Preface
In this repository, we will be studying pseudo lidar together and understanding the key ideas, preliminaries and data. I hope that after reading this respository, you will know the basics of autonomous driving perceptions (ADP) and being able to play with pseudo lidar yourself with the Kitti Dataset. Autonomous driving techniques are often not taught in school so it's difficult to learn as there are really no lectures online. We learned a tiny bit by reading research papers and we hope this tiny bit could help give you a kick start in your learning as well. To gain the most from this repository, it's required that you know a little bit about deep learning and computer vision. 

## Pseudo Lidar (PL)
PL is a great paper in 2018, which brought a revolutionary idea for 3D perceptions. It mainly contains two parts. The first part is generating pseudo lidar, which is our main focus; the second part is feeding the generated pseudo lidar into any existing 3D points cloud networks to perform 3D object detection. Here's a pipeline taken from the paper. The second part works with any existing networks so it's less important. The paper is revolutionary in a way that it closes the gap between cameras and lidars and showed that the data representation is quite important, even more important than the data quality. The only downside is the extra cost of having a depth estimation model.
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/pl.png?raw=true'>

## Preliminaries
Autnomous driving is a really hot field at the moment and there are many companies hiring engineers to work on different parts of autonomous driving, like motion planning, mapping, perception and etc. It's commonly believed that perception is the most important task as having a good perception system can make the downstream tasks much easier. There are four types of sensors often used in ADP:
* Cameras
  * Cheap
  * High resolution
  * Trouble with depth estimation
* Lidar
  * Very expensive
  * Accurate
* Radar
  * Sparse but reliable 3D information
  * Performance cannot match with Lidar
* HD Maps
  * Easy to implement and extend
  * Limited Receptive Field
  * Computationally costly
If you are not familiar with some of the terminologies, don't worry. We are sure that you can familiar with cameras as we all use cameras on a daily basis. Our pseudo lidar is also based on camera. You will see the words "monocular" and "stereo" somewhere else; they just mean single camera and two cameras (left and right).

## Data Understanding
Before we get started with the actual pseudo lidar algorithm, it's crucial to understand the data that we are working with. We will be using KITTI dataset, a very famous dataset for the development and benchmarking of autonomous driving. To work with pseudo lidar algorithm, we only need 3 types of data from the KITTI dataset, they are:
1. images taken by the left camera
2. images taken by the right camera
3. velodyne point cloud (generated by lidar, our ground truth)

Why these 3 types of data? Pseudo lidar is an algorithm that generates fake lidar point clouds from solely cameras. Therefore we need the camera data (both left and right) to formulate the input of the algorithm as well as the ground truth for the output of the algorithm to compare with. Let's visualize them in order (left, right, point cloud):
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/left.png?raw=true'>
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/right.png?raw=true'>
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/pointcloud.png?raw=true'>
Briefly speaking, we will be using the first two images to generate similar point clouds in the 3D space like shown on the third image.

## Disparity and Depth
Disparity and Depth are two main concepts in epipolar geometry and stereo imaging. The definition is quite simple. The displacement between the locations of the two features in the image plane is called the disparity. Depth is the just z-location of a point in the 3D space and can be inferred with the following formula: `depth = (baseline * focal length) / disparity)` where the baseline is the distance between the two cameras. You can get disparity with depth as well if you know high school maths. Let's visualize them.
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/depth_disp.png?raw=true'>
Since we have the point cloud data generated by lidar, we can easily get depth of each point in the image. Then we use the formula to get disparity map of the image as well. A map in computer vision means a dense estimation of each pixel in a given image. For more detail, please see the code. Sometimes one line of code is worth one thousand words!

## Train Depth Model
We have now generated the ground truth disparity map for each image and we can now start training a model that make a dense prediction for each pixel in an image. In the paper the author used PSMNetwork (Pyramid Stereo Matching Network, https://arxiv.org/abs/1803.08669). The model architecture taken from the paper is shown below. Regarding the details of this architecture, please check the original paper. For introduction purposes, we will only need to know that this model takes left and right camera images and output dense predictions (disparity) for each pixel.
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/psm.png?raw=true'>

## Generate Pseudo Lidar
We have now trainined the model. It's time to visualize our predictions (generated lidar point clouds). First let's see an original image
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/original_img.png?raw=true'>
Let's now see the generated point clouds from the our PSMNetwork:
<img src='https://github.com/naivepig1998/pseudo_lidar/blob/main/images/generated_pc.png?raw=true'>
Looks like our model works pretty well. All cars in the field of view are detected.
